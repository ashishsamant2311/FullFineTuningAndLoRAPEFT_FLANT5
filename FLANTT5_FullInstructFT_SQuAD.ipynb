{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jqps2zk-BDtU",
    "outputId": "9f191a75-2007-4158-f5a2-de42b99c2d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m481.3/491.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# transformers to load huggingface models\n",
    "# datasets to load SQuAD dataset\n",
    "# pandas for dataframe operations\n",
    "!pip install transformers datasets pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKN4llb4BAh8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435,
     "referenced_widgets": [
      "a668c39cb0f84d9e943d88c5e1cf9112",
      "16adcae6a80c45008dfb99628ed8d180",
      "ae98f9cd8dd541b29d0d8a3ea366f9d6",
      "a355f64d864b4cb78b321a661ca5e6df",
      "94604092ec934bb1bb7c09bc40f0e2bf",
      "345e28753f1a4615a1c706a65f268549",
      "7b7733d889c541e0ba6d90a8b19e49e3",
      "6ff14de8e3ed4ba88144d748b02c817b",
      "e6f52ecc4e5a44b19679c4115198cbe8",
      "cc9dec57c3064249802616c5a34cb4e2",
      "60ef4950ee88490f97fd39e2b37d8d64",
      "48dd0734ce3a44ceb3acb7d3474438ff",
      "cd65c6e9cb1d4c21bc321893839ac1ae",
      "816eb622a7cc4cf7afa3d0edb5726cf5",
      "fd806658eb824bddaed2e66efb65247d",
      "764d2b2c871b4011bdf4b1625f3bfc7c",
      "40ade28d732541fc9fa037bc22f978fb",
      "30df07a5081b4566a88fcf66406d4335",
      "6436b46d5fe8427985811d9b73059ccf",
      "519cbc6e46474b7097a3ad3765f119f8",
      "32ebf7aa6fc64ac19aab541cc2f14487",
      "0c1f84b959934f21ae440f44a2fada81",
      "fec132a4631b405f8b904c71611371ca",
      "e6132b34c3cd4f5e8aec69bded45a612",
      "d0292c71468d4041995c9ba5c55d0f12",
      "1e01528f97ab481686f6ce24d9480fa8",
      "cfcedc416c024903aa97d6a5bdbef10e",
      "51d85b28ff6641189621c5394d4d6643",
      "db46ab26d4334f6d8ddc20b19a73d1b3",
      "90aa65b8f4b0496a89b921db1cc765a9",
      "16bc0e7c84c24cd7b59ff6ec379b4159",
      "404029928d8c4dacaae7835070c5172d",
      "bfef7b01a5114a0e9cb821cdb1578307",
      "aefc452d80a34831b8d6a01e49a0dc86",
      "20968af24d7f4400acd93b78b2c64632",
      "2ba0542a7aae44bc9558372ff7367ece",
      "62729fa70ce94d7ca7783fac2101cd5e",
      "d36319a82d0a47c7a7cc6e20547556e3",
      "761718623fbb4ea69c6e422fd2629ebc",
      "be79c25533b74ef0ace8c97145bdace1",
      "799b6beaccf04b8a8b5c2b5d468cf192",
      "53ba9979ac014b8cb76dd6b156617e28",
      "e52174108b2f47ad8b97289a0ea12f50",
      "3a9cfdabd2214c039e6b9ce103ab1334",
      "8fc033d19dd54fb7a6f136d34cbbb512",
      "7f5c5b5db9124c90bd69a95358cfbad2",
      "1054e4e115c44fda882dcf79e80a5692",
      "f470eb20d6314ef08d6ccf0244008969",
      "2a98b044aa744e7c8f1d86a360907106",
      "e3bd5cda77f147fa94ca8c83ac12fcbc",
      "c99488d77f4d40caac8edb4ebce64035",
      "4da173184daf470886b05ace09feb45e",
      "e582f8092f69403ca7cdd30ee67b89ba",
      "e17b1013c91a4ff389e3502d1a38b52c",
      "9b41a64f17b446c5b09b5989d9e296ac",
      "46430cdbea074f03b327809a5c7c5274",
      "486543ca91f04b9cb639e151257b69a0",
      "8dce9ee7ee1343ad93ba3effa83b8e1c",
      "f038d865a5c74425ab680ab63c8d107f",
      "d1ecac15933e49e0b43fc45a7ba7b5d7",
      "7ec9c1106a3c44b6bf2e40ac3e234fe0",
      "c1b1c5ef9fc94110af62e8a648b62242",
      "a66f4ad35bc14de295b632fb061bc79e",
      "e1ba132ef8174301826d8b31286fd5e7",
      "28cb3225b9aa414eab5454bed1cc100d",
      "6272d59d072e4d82aa6e80f2ccfa6adb",
      "b1463a59e7e74084b1dbaddb936fa42b",
      "fda54a2cf0984679988e61eaacd18bb2",
      "42ee44201112453380ed1d0dc1aa400d",
      "0db0a706ff8246c0bc12f19edab9cde1",
      "b38b384ae5fe4e459ed7a1dafed131c4",
      "b4a148d0decf47c5b0fd1e84785e23e5",
      "b7e08406c3dc42b0948b4b63c4d798fe",
      "f566549f0be94028890f61fc1153b8a7",
      "38978c8757ea4f19a5905c515a47bf61",
      "3935466fe78840d9bdc53982ac6be9b3",
      "7a5d2807eac0437ca19e660b3d7210a5"
     ]
    },
    "id": "Kxy1CYefBbAp",
    "outputId": "e5fe8a34-4112-43a2-f3a5-afa5343ff94b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a668c39cb0f84d9e943d88c5e1cf9112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dd0734ce3a44ceb3acb7d3474438ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec132a4631b405f8b904c71611371ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefc452d80a34831b8d6a01e49a0dc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc033d19dd54fb7a6f136d34cbbb512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46430cdbea074f03b327809a5c7c5274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)b68e004ad934361fb35b9b2bd50b45ea90790fc8:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1463a59e7e74084b1dbaddb936fa42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using the FLANT5 Base model for training\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDZen-9vD1bM",
    "outputId": "8e2e66dd-1a00-4451-8fa9-87ef1f04f115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Google drive interface for saving the finetuned model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-ojohtOBmYV"
   },
   "outputs": [],
   "source": [
    "# Preprocessing function for SQuAD dataset\n",
    "# Specifying the instruction format for the Input and Output\n",
    "def preprocess_data(example):\n",
    "    example[\"input_text\"] = \"question: \" + example[\"question\"] + \" context: \" + example[\"context\"]\n",
    "    example[\"target_text\"] = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"no answer\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4E8NU6qBufo"
   },
   "outputs": [],
   "source": [
    "# Loading the SQuAD Dataset and preprocessing the train and validation portions\n",
    "# Chunking the dataset due to Google Colab GPU hour limitations\n",
    "dataset = load_dataset(\"squad\")\n",
    "train_data = dataset[\"train\"].map(preprocess_data)\n",
    "val_data = dataset[\"validation\"].map(preprocess_data)\n",
    "TRAIN_CHUNK = 25000\n",
    "TEST_CHUNK = 5000\n",
    "train_data = train_data.select(range(TRAIN_CHUNK))\n",
    "val_data = val_data.select(range(TEST_CHUNK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ytXezprB4t2"
   },
   "outputs": [],
   "source": [
    "# Specifying the max input and output lengths\n",
    "# tokenizing the inputs and saving the targets as labels in the set.\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"target_text\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "08129a9a998941ae89a0559fb25f8772",
      "90aded2352dc4def8ceffdaf86084ffb",
      "8a4e4fe334204499a5e6cc3b8e0ccf30",
      "b77520398e34424d9a9c3e0650e6bea3",
      "4def1b4e684241a5a047b862e7f2d4c6",
      "70d49624d5a04148a2f3a67015a06e63",
      "8e3322d256d8448bac0b496fa0ef7a2c",
      "c450c45cbc084e20a2e7330bb91bdf2b",
      "3eba5fd046c744cab5d5d6df33af4d2f",
      "36774befe50f499fb709c2b7f49b796b",
      "2e43a66d009b4be4a670bbb0cb98f129",
      "c6fa7addb26141329052b25682e68d63",
      "68ce82e12d0b490fba1f4c194f43d16a",
      "411635a6cb244d478e94e3d507a097b5",
      "62f0c43287f24fc6898ac3f5ce5dadb0",
      "0499f62f526e4d2b8e86ddef5673e5e2",
      "03113a02223f4323b8cd88d1b9b35100",
      "1b1017e9812242b980ac6b4ec99fc5e0",
      "e0feac7213914bd183deddaf41af152b",
      "005b83f61d534e58a26dc4258885e854",
      "9ff23999fc3741ab976f03f59b637b52",
      "631bd7264bcd41f0acdbf4bf62850668"
     ]
    },
    "id": "t2TW0VZYD9mW",
    "outputId": "0b7d62ce-ad27-430f-ee2f-792591a98894"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08129a9a998941ae89a0559fb25f8772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fa7addb26141329052b25682e68d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a tokenized version of the dataset and removing the column names\n",
    "train_dataset = train_data.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "val_dataset = val_data.map(tokenize_function, batched=True, remove_columns=dataset[\"validation\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGJo7BKWEE-U"
   },
   "outputs": [],
   "source": [
    "# if GPU is available, model loads on GPU. If not, it loads on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxuUEtQXElcm",
    "outputId": "6019329f-d0d2-4573-e8cc-9b4522152ff1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory where model checkpoints and other outputs will be saved.\n",
    "    evaluation_strategy=\"epoch\",  # Perform evaluation at the end of each epoch.\n",
    "    learning_rate=1e-4,  # The learning rate for the optimizer.\n",
    "    per_device_train_batch_size=8,  # Number of samples per device (e.g., GPU) during training.\n",
    "    per_device_eval_batch_size=8,  # Number of samples per device during evaluation.\n",
    "    num_train_epochs=3,  # Number of complete passes through the training dataset.\n",
    "    weight_decay=0.01,  # Strength of weight decay (L2 regularization) to prevent overfitting.\n",
    "    # logging_dir=\"./logs\",  # (Commented out) Directory for logging, useful for tools like TensorBoard and WandB.\n",
    "    save_strategy=\"epoch\",  # Save model checkpoints at the end of each epoch.\n",
    "    report_to=\"none\"  # Disable reporting to external tools like WandB or TensorBoard.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "PlYuOckmE1G8",
    "outputId": "a1c2e30d-9dc2-4e58-c23e-a6aecf945727"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9376' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 3:39:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.014306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.015012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='427' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [427/625 03:11 < 01:29, 2.22 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 3:45:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.014306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.015012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.015745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.012668695271809896, metrics={'train_runtime': 13506.3779, 'train_samples_per_second': 5.553, 'train_steps_per_second': 0.694, 'total_flos': 5.13568014336e+16, 'train_loss': 0.012668695271809896, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Seq2Seq trainer for training and evaluation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # The sequence-to-sequence model to be trained\n",
    "    args=training_args,  # Training arguments containing hyperparameters and configurations.\n",
    "    train_dataset=train_dataset,  # The dataset used for training the model.\n",
    "    eval_dataset=val_dataset,  # The dataset used for evaluating the model's performance during training.\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()  # Trains the model using the specified arguments, training dataset, and evaluation dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W5u5dznIx01"
   },
   "outputs": [],
   "source": [
    "# Speficying the save path on Google Drive to save the model\n",
    "save_path = \"/content/drive/MyDrive/T5/fine_tuned_t5\"\n",
    "model.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
