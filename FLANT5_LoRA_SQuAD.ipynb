{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFkOeNCeKvr3",
    "outputId": "ecc81bb7-e481-4dfe-cc9a-ec83c7cf39de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install Hugging Face Libraries\n",
    "# transfomers to load huggingface models\n",
    "# peft to perform LoRA\n",
    "# datasets to pull the SQuAD Dataset\n",
    "# evaluate to pull the evaluation metrics\n",
    "# loralib to perform LoRA\n",
    "!pip install transformers peft datasets accelerate evaluate loralib --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBc73qgehyEJ"
   },
   "outputs": [],
   "source": [
    "# Disabling the WANDB - Weights And Biases logging interface\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392,
     "referenced_widgets": [
      "b82de6b154d04a21b0678bf3731c6093",
      "56b3d09f179a4985a73d424f87602f03",
      "1c22a4063c364892a20a2772bb7b9ce0",
      "404af072691e41e29035ecb252d3e45c",
      "2493cf16123c479f8ed6947aecc6f33f",
      "cedc883ec756403cb5e21077dbb4474d",
      "4e44a188b7c844fd8d30cf4820b8ea4b",
      "9a9d7e957acf4c6a88ebd4a60ebdfbe2",
      "707b19166ca74f129b6022bb06b58520",
      "560d53ec1b984926bd57c0ca75e340fe",
      "afec5aa036f2417db13f81dd6dbf521f",
      "94d90fead4a14a18b1de4d1d468ec244",
      "7f612e91e6d549319b66b4aed877d360",
      "7ba11c230496456db605c7500ddba23c",
      "d72e7113f50b4366b93c4929ac66da8e",
      "1ad7fa9717a547549ec848dfd76d5e86",
      "1932e99769544165ae33873fc3891d75",
      "ec30c5a5edfd4e3bbf2a807bf0882e20",
      "7463aead9e3046fe9ee8d9b0dd9a0b4c",
      "f2d22d2c508a4f1a905904a75270ff7c",
      "30ee8da10be84362b9a037f594b1f22a",
      "6337f78216484f9fa42c47b54a6c2290",
      "a4c59a3c22b4405eabcd73b9ef2cf53c",
      "f54950af53fd4389bc1d9e6e502c2ba7",
      "e650d3ce91274190b5e997fd9655e46f",
      "4da39ae07c7649aab9eefb458bcf8a55",
      "d38c375c1dc64da2a5668cbc4dd6a77e",
      "3fac93576cb34238a032dcafa6422898",
      "c192881c7e834b49a0b40c8ce900074c",
      "0201450134e24d21b8480e1f50be7afc",
      "4caef661788b467b86aec40270857d53",
      "b7fa15335c6d4d73977070d26aaec1d2",
      "40700eb282034cd2a236e4f8923ac8ff",
      "2b620317c31345d9a73e93f9ca6ba003",
      "61ae1aa8702b48809bd8a61a6effdbbd",
      "16dfbcc944b34571ae8fc3072d32a8e8",
      "81b09f0b28da46e08df4cea1d07866e5",
      "36cd41cc622a4ba8b8d3903066977fae",
      "9478e32c19cd49bdbffab8b730b3478b",
      "404ea08de19245038b97a7e3a9dd39ef",
      "44378283bfaf415e81de08aac27686f6",
      "b1fa0631e02346a69a770912191e4630",
      "117e9d3f270f467ea1e65939880558ba",
      "dd650285ad5d47dbac92878cd0a98fa1",
      "4a424ed6594b425f828eeffe57def321",
      "1f7d07c96b0c47479437d75465a129e3",
      "6d5c5035c390456ea0bf833797db7c0a",
      "e1986eb58b5240259a83ed1494b4874c",
      "f0e812e0e96841118638c64e8947648f",
      "1b4c06492bcc467c8d83b41fc31bcef4",
      "8a07e565644743488cb80ee6e977f352",
      "ee7fd37beebe450597713cf43cbf31a7",
      "f32d2055e597497b988bca6571880baf",
      "b04d1b91e83a4162891a194367be2a8b",
      "e957411d365841f089752ea3048e7c1d"
     ]
    },
    "id": "OFWBP3-gK3og",
    "outputId": "63581cf9-6b8f-49ba-8033-2fcc8dc808b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82de6b154d04a21b0678bf3731c6093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d90fead4a14a18b1de4d1d468ec244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c59a3c22b4405eabcd73b9ef2cf53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b620317c31345d9a73e93f9ca6ba003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a424ed6594b425f828eeffe57def321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 87599\n",
      "Test dataset size: 10570\n"
     ]
    }
   ],
   "source": [
    "# Loading the SQuAD dataset and print the train and validation counts\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['validation'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200,
     "referenced_widgets": [
      "2501ac9fdccb4f018f85eda90ac6dced",
      "c2b31da181a94235b0394a3af588fbc3",
      "041364b924f54c89a73ecb6822e23ff3",
      "fb4d8ef010bd43398adb2812c8c506d4",
      "7cc6dec4808342fca3684d85467981a2",
      "b195f4e94f74425dbb8d460b8798ad4b",
      "c0a49d87af3b4be09fc74bb5ea76e4e9",
      "17a8e4e6243941ebb9510616c3faf33d",
      "2082cfc41db74c12b9141eb51d15f6e4",
      "5c45ec34590c4e9e8947aa3a9233e38f",
      "8a1a944864a8480989aa9381e8038644",
      "51b9aacdb7fd4317ab53c0dd0a2f5d0c",
      "a0914936460d4674ad988af79e3c8c04",
      "ba7d3be3f2c44080b4d53b5f4bb612c0",
      "42fc27ba594a4f988d1c1e666e0105db",
      "3dcfb1d8db754da7b6353d3893624655",
      "2b7d795e41024b78a3c69f6d73af7a80",
      "2368a231fbd04ede8c5064d8176b57cf",
      "6ecc6a6a93a74c4c81c0e85b2cf20cbc",
      "1cf3b51129fa48df8a123a7e136b9063",
      "c46e841a88ca4f72924f4e6eb5f8ed24",
      "4bcae2f0a19948dc879121a529dc35b0",
      "f609e49676584f95988146fb4b038ac7",
      "842f341b496f4d759a7454668f1556bf",
      "2df2743177de4196b610005ae8feef23",
      "8857800412ee4f5a8a45ddb8f88d05b5",
      "626f136f1b9e47af914bad0430d0a4ea",
      "81c5bfecfc9c42eabd50cabb7c16ac28",
      "47d83394fd1c42d9a1fa3318c5dd6ac9",
      "f8d3072d94704948b9cdeff845e8289c",
      "b4f4f6bbe97e4efdb1db543db2b900cc",
      "8d5d2a2e6df64cf39b6c25de6d494426",
      "01d0681c0a8f48eebcb84f3f88bb327c",
      "c14610cb1e6344e1b3e531ff30f2b1e3",
      "37952d3f9c3e4bafaaafe3de2fd3d78e",
      "a207f21fcdcf49dfa651f28f7453af48",
      "e65b11fe658b42b7be5d075eea21d08d",
      "e996914952f7431b9f68ff9ed625d359",
      "89903dfce7ba4713b864b84f052cbe49",
      "1d3283060d764effb7aabcf19733444a",
      "8d0350ff2e264fd9b8d9d802bb453c87",
      "6a41378f60564f909d9787c4ebdf4b5f",
      "e98ed3459eb44d3684e78c79e102ad31",
      "00fc0ad5cde249b4b73d7951fba75468"
     ]
    },
    "id": "cSIPijEhMN5S",
    "outputId": "d7d18bfa-4e0c-4364-b524-c0319754535c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2501ac9fdccb4f018f85eda90ac6dced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b9aacdb7fd4317ab53c0dd0a2f5d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f609e49676584f95988146fb4b038ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14610cb1e6344e1b3e531ff30f2b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Autotokeinzer to Load T5Tokenizer\n",
    "# AutoModelForSeq2SeqLM to load the FLAN T5 model\n",
    "# Using the FLAN T5 Model as the baseline model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-base\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjCuL7_9Ltz6",
    "outputId": "074936e4-5614-4d69-ba74-4b0545df1c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Google Drive Interfacing for saving the PEFT Adapter\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "5bbe14b2e93045dea8a02c4509241218",
      "fc1d9df18e4f4349939d2eb9dcaee4b6",
      "f98f743795e9455eb565e586214cc0f1",
      "fc92da6ce9d6404fbd855a0b4457c927",
      "98208e8fde5f4e359a73829bd7214c12",
      "f7868b52e9b74ac4b19f3cc08d85a822",
      "a049613f489b4014a4e7a39c5d628870",
      "4ebd91fdf70d434682a598e81aab0ad1",
      "6d0df75ab5ba414a8df3eb352c32be3e",
      "fb87e55506934d17a9d0324195bc737e",
      "4a2ff0ccba8a4b9db0ec4ce305cada1d",
      "b4b711a3f8ca4ccba640f743a2c57001",
      "168a693ac57d4cfa8e90662604e62e68",
      "623f8f638f644b8685550a62ac6c3db7",
      "8f1503b39f17440cbeb6825f74772615",
      "5009752875354799a6a90564ce21c1dc",
      "82ebc8609ca34f808ab8d13437717fa0",
      "624748f3de48478f9b4de6996575586f",
      "0ab4230c83ef4690bee6af17fc1ce903",
      "066193b6893749d4bc7906152fc59608",
      "3ecc85a9c0f64dc1bb4ffb4699bb98ae",
      "813d33a8ae6049c1be0fb74c7d0fd43a"
     ]
    },
    "id": "no8qYHLQMbUs",
    "outputId": "029739dc-d062-40b4-8ad7-a53d310a2aa2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbe14b2e93045dea8a02c4509241218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b711a3f8ca4ccba640f743a2c57001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 512\n"
     ]
    }
   ],
   "source": [
    "# Concatenate_datasets is used to concat two datasets\n",
    "# numpy for numerical operations\n",
    "# This code block can essentially be eliminated. It uses a lot of compute and the\n",
    "# aim of this code is to ensure that the max_length used in the SQuAD dataset is\n",
    "# compatible with the generation and context window of the FLAN T5\n",
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "# Set a target maximum length (e.g., 512)\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Function to preprocess inputs\n",
    "def preprocess_inputs(examples):\n",
    "    # Concatenate context and question as a single input string\n",
    "    inputs = [context + \" \" + question for context, question in zip(examples[\"context\"], examples[\"question\"])]\n",
    "    # Tokenize with truncation and padding\n",
    "    return tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,  # Limit the tokenized sequence length\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Ensure consistent length by padding to MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Concatenate datasets and tokenize inputs\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(\n",
    "    preprocess_inputs,\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"question\", \"answers\"]  # Remove original columns to avoid conflict\n",
    ")\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "max_source_length = int(np.percentile(input_lengths, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# Function to preprocess targets (answers)\n",
    "def preprocess_targets(examples):\n",
    "    # Extract the first answer from the `text` field, if available; otherwise, use an empty string\n",
    "    answers = [ans[\"text\"][0] if len(ans[\"text\"]) > 0 else \"\" for ans in examples[\"answers\"]]\n",
    "    # Tokenize with truncation and padding\n",
    "    return tokenizer(\n",
    "        answers,\n",
    "        max_length=MAX_LENGTH,  # Limit tokenized answer length\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Ensure consistent length by padding to MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Tokenize targets\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(\n",
    "    preprocess_targets,\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"question\", \"answers\"]\n",
    ")\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "max_target_length = int(np.percentile(target_lengths, 90))\n",
    "print(f\"Max target length: {max_target_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "f5309e4e1d3e44d7b02ef78d51d0a8ad",
      "7b202ad354284bdfb617113b8c274c48",
      "3f1cb1ef09bd430a9d78bde9c2208749",
      "129029d4467c4bd3aa1685bebedd269e",
      "eb1e38793ea44cd489b88be6bba64c85",
      "48bd5c60be804c8885bb4cfcf9bf813d",
      "b6be4ba7e1d84a1e9b7179acd0b656d3",
      "84fe4067011a431fb23d63f7579def41",
      "c34daf995a5e4b10aba8740a0bddc713",
      "90201407adca4605abc74e9b62e54b04",
      "5b31ac8d40a0433c8de4217024f3a8dc",
      "57dee327069940e9be4e8f52dcd7bb5d",
      "1750aa00f0bb488cae1e13cd44f3a74f",
      "8ec6282d4bae4e23b65256aed47aedd5",
      "cb2fef58f93445da8829a094ab57137a",
      "f598b6440bb94272b46e2382c052ce28",
      "4d3e157f9dbb4acaac6c694fc514542d",
      "298ec327f1e74d4fb697aa4b6259cf46",
      "7bf9bb223efa4755ba5393eaae53dbf6",
      "ed64bb3c9f5948b2822f235809113a0b",
      "fc411d26f1dc4bbfbb09194e30d08eec",
      "567e4add231e4a9285e3f4722a12cc19",
      "4311a0f230ac49e599d64c2e11618fe7",
      "1df5f363743343099a38f7dc6e9d23d0",
      "bea99f7f164547f8bae8d18328d5c80c",
      "b4319f86935246269b3c562e3431b2b6",
      "65fd1e2679c64323b62782b92c2ca0d1",
      "b6c6551c0f984603b08bd11e7c5e5e5a",
      "60710e1beef54e33b8c748a96df1ad2f",
      "79e632b41df34498a9804c64e0b7ad46",
      "8aed1d504ede4cb98dfe38976bf2762d",
      "2e76586bb7924465aa7dbbec0513819e",
      "40c094ab22774941b2ffe89770cdd7af",
      "95cb3ad4c751439392598568583d9aab",
      "2a596d52056b4e5894f3a7e0dd6759dc",
      "b97c0638424b45d791e78d8561ca0a84",
      "4f16726cf2e14275ab825319f3bf87d0",
      "71e7e6427d984f80a2a63f2ef8c08ae5",
      "722fd8ac961f40279ada7035765445ad",
      "f2e2f9aaee9542ea83797a92cfc76c3d",
      "34ea3a580ff448869e53c0af854cdfc1",
      "d2c6d39b63324182a872f921240ea3e1",
      "285b055273c24a9abeedd27e81238646",
      "dd884a79c2dc4e389dd755fe45179323"
     ]
    },
    "id": "iB_GGZjYU5wy",
    "outputId": "602808b3-6b9b-4017-a5c8-997083d781c0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5309e4e1d3e44d7b02ef78d51d0a8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dee327069940e9be4e8f52dcd7bb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4311a0f230ac49e599d64c2e11618fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cb3ad4c751439392598568583d9aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['title', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Actual pre-processing function for converting text to tokens and creating\n",
    "# the output tokens. Output tokens are saved as labels.\n",
    "# -100 token id tells the loss function not to consider it for loss calculation\n",
    "# as it is the tokenid for padding token.\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # Construct inputs by combining context and question\n",
    "    inputs = [f\"question: {q} context: {c}\" for q, c in zip(sample[\"question\"], sample[\"context\"])]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Extract the first answer text as the target\n",
    "    targets = [ans[\"text\"][0] if len(ans[\"text\"]) > 0 else \"\" for ans in sample[\"answers\"]]\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Replace `pad_token_id` with `-100` to ignore padding in the loss calculation\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    # Add labels to the inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Map the function over the dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"context\", \"question\", \"answers\", \"id\"]  # Adjust columns for SQuAD\n",
    ")\n",
    "\n",
    "# Save tokenized datasets\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"validation\"].save_to_disk(\"data/eval\")\n",
    "\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RI9ApolKopXJ"
   },
   "outputs": [],
   "source": [
    "# Due to Google Colab limits, taken a 10000 record sample for training and 2000 record sample for validation\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "val_dataset = tokenized_dataset[\"validation\"].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "8d8bc989812d4948899c01e4edbfd8a5",
      "25fd96149d024baeb6e91f40bf476f16",
      "5e83dcf9411441a5b2e9775fe1027b44",
      "0ad7506c9fb34ac1b52f463d42e34dce",
      "0f1435eea8ae49428597ccf2774e44d7",
      "9317e4423fce4f52804110f0ac60cd13",
      "ee8d92afbc334adc83a37a34d0dedb5e",
      "3cd1ef28db1a43fe9c32b3879e3135cd",
      "10eaba8898ae45cdb761634555d82417",
      "5430eb62e8664ebc979ec3717f180f42",
      "29b03d10c27d494cb712b23181438ffd",
      "8fbb7e8df8134b17b7fa19699d93aba5",
      "2c2c0e84a8314cd88a72890bc2267f8c",
      "b08df2b4e68b4297b5c4e600c37e2df9",
      "d235791555fe4d2aab485c55b322fef6",
      "4425bb43542d49e7b7ac175ad5a40a40",
      "8f6d4e72542d4858b6d6a4c11cb95f22",
      "c09b558a99cc4df6985d6e1bab48e789",
      "231e4565fb15413b9bb5ab1838a1393e",
      "462ae2a0a588475c9f53c483b40ed4fe",
      "e573081b3b744aec866be2986cf36cf0",
      "f5b9fe4bd405472486e1b34ea746febe",
      "2b06e5e2a6044bc5bad02a7f1d660683",
      "ebc900f282564a04a693b6e735302c3f",
      "ae715256303a470fb788ceef8e3a8f08",
      "8ccf6062da6c4dc1a55eb86097a62f23",
      "f258151cc0d74371bd8f0223705439b7",
      "2dfdc57178ab4512b56f07a4d0278ecd",
      "72f7c2ba91fd4905ae5ae94473586c70",
      "11155925624844da910552620f6691e6",
      "61d0a07018724b01956485e9698fcbcc",
      "0dab7b4b39b140659638291c5a68bb1d",
      "eaeda58fd8e04ba48ab2321e1ce73597"
     ]
    },
    "id": "agx4MFHyWBXH",
    "outputId": "42dd5b89-536a-41ba-8b2c-0915efa628d6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8bc989812d4948899c01e4edbfd8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbb7e8df8134b17b7fa19699d93aba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b06e5e2a6044bc5bad02a7f1d660683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model from the hub\n",
    "# device_map - to load the model on GPU if available, else CPU\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t6GPl2iWD67",
    "outputId": "590c69b3-e8b4-4798-f396-2bfe1ce2cb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,462,592 || trainable%: 0.3561\n"
     ]
    }
   ],
   "source": [
    "# LoraConfig to provide training configuration for LoRA\n",
    "# get_peft_model to freeze the base model and load adapter on top of it.\n",
    "# TaskType to inform the config about the kind of task, here SEQ2SEQ LM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuration for Low-Rank Adaptation (LoRA) of neural networks\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the LoRA adaptation matrices; controls the size of the learned parameters.\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA updates; balances between LoRA updates and base model weights.\n",
    "    target_modules=[\"q\", \"v\"],  # Specifies which modules to apply LoRA on (e.g., query \"q\" and value \"v\" matrices in transformers).\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA layers; helps prevent overfitting during training.\n",
    "    bias=\"none\",  # Specifies how biases are handled; \"none\" means no biases are modified by LoRA.\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Task type; \"SEQ_2_SEQ_LM\" indicates the model is for sequence-to-sequence language modeling.\n",
    ")\n",
    "\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "# prints the trainable parameters of the model\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSqdYn5WWnEp"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# The ID used for padding labels; -100 is commonly used as it ensures the pad token is ignored in the loss calculation.\n",
    "label_pad_token_id = -100\n",
    "\n",
    "# Creating a data collator for sequence-to-sequence tasks.\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,  # The tokenizer used to preprocess the data for the model.\n",
    "    model=model,  # The model for which the data is being prepared.\n",
    "    label_pad_token_id=label_pad_token_id,  # Specifies the padding token ID for labels, ensuring ignored positions during loss computation.\n",
    "    pad_to_multiple_of=8  # Ensures the sequences are padded to a multiple of 8 for efficiency on GPUs with tensor cores.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8lneDT8WzLt"
   },
   "outputs": [],
   "source": [
    "# Seq2SeqTrainer is the trainer adaptation for SEQ2SEQ LMs\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Define training arguments for sequence-to-sequence training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-lora-squad\",  # Directory to save model checkpoints and logs.\n",
    "    num_train_epochs=3,  # Number of epochs to train the model.\n",
    "    per_device_train_batch_size=4,  # Batch size per device during training.\n",
    "    per_device_eval_batch_size=4,  # Batch size per device during evaluation.\n",
    "    fp16=False,  # Whether to use 16-bit floating point precision for faster training. Must be False for T5\n",
    "    gradient_accumulation_steps=8,  # Number of steps to accumulate gradients before updating weights.\n",
    "    eval_strategy=\"epoch\",  # Frequency of evaluation; \"epoch\" means after every epoch.\n",
    "    save_strategy=\"epoch\",  # Frequency of saving the model; \"epoch\" means after every epoch.\n",
    "    logging_strategy=\"epoch\",  # Frequency of logging; \"epoch\" means after every epoch.\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer.\n",
    "    save_total_limit=1,  # Maximum number of checkpoints to keep; older ones will be deleted.\n",
    "    predict_with_generate=True,  # Enables text generation during prediction (important for Seq2Seq tasks).\n",
    "    report_to=[],  # Disables reporting to external tools like WandB or TensorBoard.\n",
    "    label_names=[\"labels\"],  # Specifies the key used for labels in the dataset.\n",
    ")\n",
    "\n",
    "# Initializing the Seq2Seq trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # The model to be trained.\n",
    "    args=training_args,  # Training arguments defined above.\n",
    "    eval_dataset=val_dataset,  # Validation dataset for evaluation during training.\n",
    "    data_collator=data_collator,  # Data collator for handling padding and batching.\n",
    "    train_dataset=train_dataset  # Training dataset used to train the model.\n",
    ")\n",
    "\n",
    "# Disable caching during training to prevent warnings; re-enable caching for inference for better performance.\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "-HffY84rXRT4",
    "outputId": "66778ff4-c18a-4436-87d2-de5970307cde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 2:12:41, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.318747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.313855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=936, training_loss=0.38430471501798713, metrics={'train_runtime': 7970.2155, 'train_samples_per_second': 3.764, 'train_steps_per_second': 0.117, 'total_flos': 2.05692598222848e+16, 'train_loss': 0.38430471501798713, 'epoch': 2.992})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suoQRhM0kxEY"
   },
   "outputs": [],
   "source": [
    "# Save the PEFT Adapter\n",
    "save_path = \"/content/drive/MyDrive/T5/PEFT_FLANT5_SQUAD\"\n",
    "trainer.model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIhokmgFzzTu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
